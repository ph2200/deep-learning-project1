# -*- coding: utf-8 -*-
"""mini-project1-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKIv2_2BKVFxcwSPPE6aIcgvjD75M7iF
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import matplotlib.pyplot as plt
import torch.nn.functional as F
# %matplotlib inline

"""1. Download the grayscale image dataset called CIFAR-10."""

from torchvision.transforms import transforms
cifar_norm_mean = (0.49139968, 0.48215841, 0.44653091)
cifar_norm_std = (0.24703223, 0.24348513, 0.26158784)
transform_train = torchvision.transforms.Compose([
                    torchvision.transforms.RandomCrop(32, padding=4),
                    torchvision.transforms.RandomHorizontalFlip(),
                    torchvision.transforms.ToTensor(),
                    torchvision.transforms.Normalize(cifar_norm_mean, cifar_norm_std),
                ])
transform_test = torchvision.transforms.Compose([
                    torchvision.transforms.ToTensor(),
                    torchvision.transforms.Normalize(cifar_norm_mean, cifar_norm_std),
                ])
trainingdata1 = torchvision.datasets.CIFAR10('./CIFAR10/',train=True,download=True,transform=transform_train)
trainingdata2 = torchvision.datasets.CIFAR10('./CIFAR10/',train=True,download=True,transform=transform_test)
trainingdata = torch.utils.data.ConcatDataset([trainingdata1, trainingdata2])
testdata = torchvision.datasets.CIFAR10('./CIFAR10/',train=False,download=True,transform=transform_test)

# trainingdata = torchvision.datasets.CIFAR10('./CIFAR10/',train=True,download=True,transform=torchvision.transforms.ToTensor())
# testdata = torchvision.datasets.CIFAR10('./CIFAR10/',train=False,download=True,transform=torchvision.transforms.ToTensor())

print(len(trainingdata))
print(len(testdata))

image, label = trainingdata[5]
print(image.shape, label)

"""We need to construct a data loader object, and then we can use this to train our model."""

trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=64,shuffle=True)
testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=64,shuffle=False)

"""We can use the DataLoader object to spit out a few images from the dataset."""

images, labels = iter(trainDataLoader).next()

plt.figure(figsize=(10,4))
for index in np.arange(0,5):
  plt.subplot(1,5,index+1)
  plt.imshow(np.stack((images[index][0], images[index][1], images[index][2]), axis=2))

"""2. Set up my model"""

class BasicBlock(nn.Module):
    # expansion = 1
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, input):
        residual = input
        x = self.conv1(input)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        if self.downsample:
            residual = self.downsample(residual)
        x += residual
        x = self.relu(x)
        return x

class Resnet(nn.Module):
    # 32*32
    def __init__(self, block, num_layer, n_classes=10, input_channels=3):
        super(Resnet, self).__init__()
        self.in_channels = 42
        self.conv1 = nn.Conv2d(input_channels, 42, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(42)
        # self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(block, 42, num_layer[0])
        self.layer2 = self._make_layer(block, 84, num_layer[1], 2)
        self.layer3 = self._make_layer(block, 168, num_layer[2], 2)
        self.layer4 = self._make_layer(block, 336, num_layer[3], 2)
        self.avgpool = nn.AvgPool2d(kernel_size=4, stride=1)
        self.fc = nn.Linear(336, n_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)


    def _make_layer(self, block, out_channels, num_block, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels
        for _ in range(1, num_block):
            layers.append(block(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, input):
        x = self.conv1(input)
        x = self.bn1(x)
        x = self.relu(x)
        # x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

def project1_model(**kwargs):
    """Constructs a ResNet-18 model.

    """
    model = Resnet(BasicBlock, [2, 2, 2, 2], **kwargs)
    return model

model = project1_model().cuda()

print(model)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(count_parameters(model))

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
save_loss = {'train':[], 'val':[]}
save_acc = {'train':[], 'val':[]}

for epoch in range(40):

    # Each epoch has a training and validation phase
    model.train()
    current_loss = 0.0
    current_corrects = 0
    for batch_idx, (inputs, labels) in enumerate(trainDataLoader):
      inputs = inputs.cuda()
      labels = labels.cuda()
      optimizer.zero_grad()

      with torch.set_grad_enabled(True):
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

      current_loss += loss.item() * inputs.size(0)
      current_corrects += torch.sum(preds == labels.data)

    save_loss['train'] += [current_loss / len(trainDataLoader.dataset)]
    save_acc['train'] += [current_corrects.float() / len(trainDataLoader.dataset)]
    # pretty print
    print(f"Epoch:{epoch} -- Phase:{'train'} -- Loss:{save_loss['train'][-1]:.2f} -- Acc:{save_acc['train'][-1]*100:.2f}")

    model.eval()
    current_loss = 0.0
    current_corrects = 0
    for batch_idx, (inputs, labels) in enumerate(testDataLoader):
      inputs = inputs.cuda()
      labels = labels.cuda()
      optimizer.zero_grad()

      with torch.set_grad_enabled(False):
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

      current_loss += loss.item() * inputs.size(0)
      current_corrects += torch.sum(preds == labels.data)

    save_loss['val'] += [current_loss / len(testDataLoader.dataset)]
    save_acc['val'] += [current_corrects.float() / len(testDataLoader.dataset)]
    # pretty print
    print(f"Epoch:{epoch} -- Phase:{'val'} -- Loss:{save_loss['val'][-1]:.2f} -- Acc:{save_acc['val'][-1]*100:.2f}")

plt.plot(torch.tensor(save_acc['train']))
plt.plot(torch.tensor(save_acc['val']))
plt.legend(["train", "test"])
plt.title("Accuracy with Data Augmentation")
plt.grid()

plt.plot(torch.tensor(save_loss['train']))
plt.plot(torch.tensor(save_loss['val']))
plt.legend(["train", "test"])
plt.title("Loss with Data Augmentation")
plt.grid()

# save model
model_path = './project1_model_final.pt'
torch.save(model.state_dict(), model_path)

